{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hobjuKSsY7d5",
        "outputId": "98649207-523c-4b10-f786-f5ebf742c2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-026b08a8938a>:48: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df['Outcome Time'] = pd.to_datetime(df['Outcome Time'], errors='coerce')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 7ms/step - accuracy: 0.5345 - loss: 1.1259 - val_accuracy: 0.5932 - val_loss: 1.0228\n",
            "Epoch 2/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 7ms/step - accuracy: 0.6575 - loss: 0.8701 - val_accuracy: 0.6539 - val_loss: 0.8969\n",
            "Epoch 3/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 7ms/step - accuracy: 0.6847 - loss: 0.8006 - val_accuracy: 0.6244 - val_loss: 0.9660\n",
            "Epoch 4/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 6ms/step - accuracy: 0.6991 - loss: 0.7652 - val_accuracy: 0.6380 - val_loss: 0.9505\n",
            "Epoch 5/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - accuracy: 0.7098 - loss: 0.7429 - val_accuracy: 0.6494 - val_loss: 0.9218\n",
            "Epoch 6/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 7ms/step - accuracy: 0.7180 - loss: 0.7212 - val_accuracy: 0.6456 - val_loss: 0.9292\n",
            "Epoch 7/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 7ms/step - accuracy: 0.7232 - loss: 0.7097 - val_accuracy: 0.6426 - val_loss: 0.9519\n",
            "Epoch 8/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 7ms/step - accuracy: 0.7273 - loss: 0.6995 - val_accuracy: 0.6541 - val_loss: 0.9380\n",
            "Epoch 9/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 7ms/step - accuracy: 0.7298 - loss: 0.6914 - val_accuracy: 0.6452 - val_loss: 0.9593\n",
            "Epoch 10/10\n",
            "\u001b[1m6881/6881\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 7ms/step - accuracy: 0.7335 - loss: 0.6870 - val_accuracy: 0.6721 - val_loss: 0.9103\n",
            "\u001b[1m695/695\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step\n",
            "Balanced Accuracy Score: 0.5684\n",
            "Predictions have been saved to 'predictions_nn.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load data\n",
        "df_train = pd.read_csv('train.csv')\n",
        "df_test = pd.read_csv('test.csv')\n",
        "\n",
        "def convert_age_to_days(age_str):\n",
        "    if pd.isnull(age_str) or not isinstance(age_str, str):\n",
        "        return np.nan\n",
        "    num, unit = age_str.split()[:2]\n",
        "    num = int(num)\n",
        "    if 'year' in unit:\n",
        "        return num * 365\n",
        "    elif 'month' in unit:\n",
        "        return num * 30\n",
        "    elif 'week' in unit:\n",
        "        return num * 7\n",
        "    elif 'day' in unit:\n",
        "        return num\n",
        "    return np.nan\n",
        "\n",
        "def bucket_age(days):\n",
        "    if pd.isnull(days):\n",
        "        return 'Unknown'\n",
        "    elif days < 180:\n",
        "        return 'Baby'\n",
        "    elif days < 730:\n",
        "        return 'Young'\n",
        "    elif days < 2555:\n",
        "        return 'Adult'\n",
        "    else:\n",
        "        return 'Senior'\n",
        "\n",
        "def is_in_austin_travis(location):\n",
        "    if pd.isnull(location):\n",
        "        return 0\n",
        "    location = location.lower()\n",
        "    return int('austin' in location or 'travis' in location)\n",
        "\n",
        "def preprocess_data(df, is_train=True):\n",
        "    df['AgeInDays'] = df['Age upon Intake'].apply(convert_age_to_days)\n",
        "    df['AgeGroup'] = df['AgeInDays'].apply(bucket_age)\n",
        "    df['Intake Time'] = pd.to_datetime(df['Intake Time'], errors='coerce')\n",
        "    df['Outcome Time'] = pd.to_datetime(df['Outcome Time'], errors='coerce')\n",
        "    df['IntakeHour'] = df['Intake Time'].dt.hour\n",
        "    df['IntakeMonth'] = df['Intake Time'].dt.month\n",
        "    df['IntakeWeekday'] = df['Intake Time'].dt.dayofweek\n",
        "    df['StayDuration'] = (df['Outcome Time'] - df['Intake Time']).dt.days if is_train else np.nan\n",
        "    df['IsNamed'] = df['Name'].notnull().astype(int)\n",
        "    df['IsMixedBreed'] = df['Breed'].str.contains(\"Mix\", case=False, na=False).astype(int)\n",
        "    df['Found_In_Austin_Travis'] = df['Found Location'].apply(is_in_austin_travis)\n",
        "    df['AgeInDays'] = df['AgeInDays'].fillna(df['AgeInDays'].median())\n",
        "    df['StayDuration'] = df['StayDuration'].fillna(df['StayDuration'].median())\n",
        "    if is_train:\n",
        "        df = df.dropna(subset=['Outcome Type'])\n",
        "    return df\n",
        "\n",
        "df_train = preprocess_data(df_train, is_train=True)\n",
        "df_test = preprocess_data(df_test, is_train=False)\n",
        "\n",
        "features = [\n",
        "    'AgeGroup', 'StayDuration', 'IntakeHour', 'IntakeMonth', 'IntakeWeekday',\n",
        "    'IsNamed', 'IsMixedBreed', 'Found_In_Austin_Travis',\n",
        "    'Sex upon Intake', 'Animal Type', 'Intake Condition', 'Intake Type', 'Breed'\n",
        "]\n",
        "target = 'Outcome Type'\n",
        "\n",
        "X_train = df_train[features]\n",
        "y_train = df_train[target]\n",
        "X_test = df_test[features]\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train = label_encoder.fit_transform(y_train)\n",
        "\n",
        "categorical_features = [\n",
        "    'AgeGroup', 'Sex upon Intake', 'Animal Type',\n",
        "    'Intake Condition', 'Intake Type', 'Breed'\n",
        "]\n",
        "\n",
        "numerical_features = [\n",
        "    'StayDuration', 'IntakeHour', 'IntakeMonth',\n",
        "    'IntakeWeekday', 'IsNamed', 'IsMixedBreed', 'Found_In_Austin_Travis'\n",
        "]\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "        ('num', numeric_transformer, numerical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "X_train_processed = preprocessor.fit_transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=len(label_encoder.classes_))\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Input(shape=(X_train_processed.shape[1],)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train_processed, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "y_pred_probs = model.predict(X_test_processed)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "test_predictions_df = pd.DataFrame({\n",
        "    'id': range(1, len(y_pred) + 1),\n",
        "    'Outcome Type': label_encoder.inverse_transform(y_pred)\n",
        "})\n",
        "\n",
        "test_predictions_df.to_csv('test_predictions_nn.csv', index=False)\n",
        "print(\"Test predictions have been saved to 'test_predictions_nn.csv'.\")"
      ]
    }
  ]
}