{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Balance dataset"
      ],
      "metadata": {
        "id": "0BO9zJ39QVjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "# Load datasets\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Function to convert age strings to buckets\n",
        "def bucket_age(age_str):\n",
        "    \"\"\"Convert age strings (e.g., '2 years', '4 weeks') into age buckets.\"\"\"\n",
        "    if pd.isna(age_str):\n",
        "        return \"Unknown\"\n",
        "\n",
        "    age_str = age_str.lower().strip()\n",
        "    conversion = {\"year\": 365, \"month\": 30, \"week\": 7, \"day\": 1}\n",
        "\n",
        "    match = re.match(r\"(\\d+)\\s*(year|month|week|day)s?\", age_str)\n",
        "    if match:\n",
        "        num = int(match.group(1))\n",
        "        unit = match.group(2)\n",
        "        days = num * conversion[unit]\n",
        "\n",
        "        if days <= 180:\n",
        "            return \"Baby\"\n",
        "        elif 181 <= days <= 730:\n",
        "            return \"Child\"\n",
        "        elif 1096 <= days < 4015:\n",
        "            return \"Adult\"\n",
        "        elif days >= 4015:\n",
        "            return \"Senior\"\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "# Apply age bucketing to the training set\n",
        "train_df[\"Age Bucket\"] = train_df[\"Age upon Intake\"].apply(bucket_age)\n",
        "\n",
        "# Clean Breed column: remove \"mix\" and extra whitespace\n",
        "train_df['Breed_Clean'] = train_df['Breed'].str.lower().str.replace('mix', '').str.strip()\n",
        "\n",
        "# Process Color column: convert to lowercase, remove extra spaces, and split if two colors are provided\n",
        "train_df['Color_Clean'] = train_df['Color'].str.lower().str.strip()\n",
        "color_split = train_df['Color_Clean'].str.split('/', expand=True)\n",
        "train_df['Primary_Color'] = color_split[0]\n",
        "train_df['Secondary_Color'] = color_split[1] if color_split.shape[1] > 1 else np.nan\n",
        "\n",
        "# Drop unnecessary columns\n",
        "columns_to_drop = ['Name', 'Id', 'Intake Time', 'Outcome Time', 'Age upon Intake', 'Date of Birth', 'Color',\n",
        "                   'Found Location', 'Breed']\n",
        "train_df.drop(columns=[col for col in columns_to_drop if col in train_df.columns], inplace=True)\n",
        "\n",
        "# Group rare categories into \"Other\"\n",
        "min_count = 50  # Adjust this threshold as needed\n",
        "rare_conditions = train_df['Intake Condition'].value_counts()[train_df['Intake Condition'].value_counts() < min_count].index\n",
        "train_df['Intake Condition'] = train_df['Intake Condition'].replace(rare_conditions, 'Other')\n",
        "\n",
        "# Balance the dataset\n",
        "adopted = train_df[train_df[\"Outcome Type\"] == \"Adoption\"]\n",
        "transferred = train_df[train_df[\"Outcome Type\"] == \"Transfer\"]\n",
        "euthanasia = train_df[train_df[\"Outcome Type\"] == \"Euthanasia\"]\n",
        "return_to_owner = train_df[train_df[\"Outcome Type\"] == \"Return to Owner\"]\n",
        "died = train_df[train_df[\"Outcome Type\"] == \"Died\"]\n",
        "\n",
        "minority_class = pd.concat([euthanasia, return_to_owner, died])\n",
        "target_size = len(minority_class)\n",
        "\n",
        "adopted_undersampled = resample(adopted, replace=False, n_samples=target_size, random_state=42)\n",
        "transferred_undersampled = resample(transferred, replace=False, n_samples=target_size, random_state=42)\n",
        "\n",
        "balanced_train_df = pd.concat([adopted_undersampled, transferred_undersampled, minority_class])\n",
        "\n",
        "# Encode categorical labels\n",
        "label_encoder = LabelEncoder()\n",
        "balanced_train_df[\"Outcome Type\"] = label_encoder.fit_transform(balanced_train_df[\"Outcome Type\"])\n",
        "\n",
        "# Convert categorical variables to numerical\n",
        "balanced_train_df = pd.get_dummies(balanced_train_df)\n",
        "\n",
        "# Prepare features and labels\n",
        "X = balanced_train_df.drop(columns=[\"Outcome Type\"])\n",
        "y = balanced_train_df[\"Outcome Type\"]\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build a fully connected neural network model\n",
        "model_nn = keras.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(len(label_encoder.classes_), activation='softmax')  # Multi-class classification\n",
        "])\n",
        "\n",
        "# Compile the neural network model\n",
        "model_nn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the neural network\n",
        "history = model_nn.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Build and train a Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate models\n",
        "train_accuracy_nn = history.history['accuracy'][-1]\n",
        "val_accuracy_nn = history.history['val_accuracy'][-1]\n",
        "print(f\"Neural Network - Final Training Accuracy: {train_accuracy_nn:.4f}\")\n",
        "print(f\"Neural Network - Final Validation Accuracy: {val_accuracy_nn:.4f}\")\n",
        "\n",
        "train_accuracy_rf = rf_model.score(X_train, y_train)\n",
        "val_accuracy_rf = rf_model.score(X_val, y_val)\n",
        "print(f\"Random Forest - Final Training Accuracy: {train_accuracy_rf:.4f}\")\n",
        "print(f\"Random Forest - Final Validation Accuracy: {val_accuracy_rf:.4f}\")\n",
        "\n",
        "# Preprocess test dataset\n",
        "test_df[\"Age Bucket\"] = test_df[\"Age upon Intake\"].apply(bucket_age)\n",
        "\n",
        "# Clean Breed column: remove \"mix\" and extra whitespace\n",
        "test_df['Breed_Clean'] = test_df['Breed'].str.lower().str.replace('mix', '').str.strip()\n",
        "\n",
        "# Drop unnecessary columns only if they exist\n",
        "test_df.drop(columns=[col for col in columns_to_drop if col in test_df.columns], inplace=True)\n",
        "\n",
        "# Group rare categories into \"Other\" in the test set\n",
        "test_df['Intake Condition'] = test_df['Intake Condition'].replace(rare_conditions, 'Other')\n",
        "\n",
        "# Convert categorical variables to numerical (align with training set)\n",
        "test_df = pd.get_dummies(test_df)\n",
        "test_df = test_df.reindex(columns=X.columns, fill_value=0)  # Ensure same columns as training data\n",
        "test_df_scaled = scaler.transform(test_df)\n",
        "\n",
        "# Make predictions with both models\n",
        "predictions_nn = model_nn.predict(test_df_scaled)\n",
        "predicted_classes_nn = predictions_nn.argmax(axis=1)\n",
        "\n",
        "predictions_rf = rf_model.predict(test_df_scaled)\n",
        "\n",
        "# Combine predictions using soft voting (average of predicted probabilities)\n",
        "# For soft voting, we'll average the predicted probabilities of both models.\n",
        "# Make predictions with both models\n",
        "predictions_nn = model_nn.predict(test_df_scaled)  # This gives the probabilities for each class\n",
        "predicted_classes_nn = predictions_nn.argmax(axis=1)\n",
        "\n",
        "predictions_rf = rf_model.predict(test_df_scaled)  # Random Forest also gives class predictions, not probabilities\n",
        "\n",
        "# Combine predictions using soft voting (average of predicted probabilities)\n",
        "# For soft voting, we'll average the predicted probabilities of both models.\n",
        "pred_probs_nn = model_nn.predict(test_df_scaled)  # Keras model's predictions give probabilities (softmax output)\n",
        "pred_probs_rf = rf_model.predict_proba(test_df_scaled)  # Random Forest provides probabilities\n",
        "\n",
        "# Average the predicted probabilities\n",
        "avg_probs = (pred_probs_nn + pred_probs_rf) / 2\n",
        "final_pred_classes = np.argmax(avg_probs, axis=1)  # Get the class with the highest average probability\n",
        "\n",
        "# Convert numeric predictions back to the original class labels\n",
        "final_pred_classes = label_encoder.inverse_transform(final_pred_classes)\n",
        "\n",
        "# Store predictions in test dataframe with specific column format\n",
        "test_predictions = pd.DataFrame({\"Id\": range(1, len(final_pred_classes) + 1), \"Outcome Type\": final_pred_classes})\n",
        "\n",
        "# Save predictions\n",
        "test_predictions.to_csv(\"test_predictions.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgZiwhOmTCLO",
        "outputId": "c4cf9bea-f4d3-4353-b517-707d9400e564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.4884 - loss: 1.2195 - val_accuracy: 0.5303 - val_loss: 1.0655\n",
            "Epoch 2/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5688 - loss: 0.9907 - val_accuracy: 0.5377 - val_loss: 1.0714\n",
            "Epoch 3/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5839 - loss: 0.9546 - val_accuracy: 0.5381 - val_loss: 1.0908\n",
            "Epoch 4/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5889 - loss: 0.9384 - val_accuracy: 0.5482 - val_loss: 1.0824\n",
            "Epoch 5/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.5944 - loss: 0.9224 - val_accuracy: 0.5450 - val_loss: 1.0834\n",
            "Epoch 6/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5989 - loss: 0.9131 - val_accuracy: 0.5469 - val_loss: 1.0920\n",
            "Epoch 7/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6109 - loss: 0.8971 - val_accuracy: 0.5431 - val_loss: 1.1034\n",
            "Epoch 8/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6034 - loss: 0.8987 - val_accuracy: 0.5397 - val_loss: 1.1383\n",
            "Epoch 9/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6154 - loss: 0.8837 - val_accuracy: 0.5428 - val_loss: 1.1502\n",
            "Epoch 10/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.6192 - loss: 0.8705 - val_accuracy: 0.5466 - val_loss: 1.1880\n",
            "Epoch 11/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6254 - loss: 0.8617 - val_accuracy: 0.5400 - val_loss: 1.1803\n",
            "Epoch 12/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6282 - loss: 0.8545 - val_accuracy: 0.5432 - val_loss: 1.1869\n",
            "Epoch 13/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6284 - loss: 0.8470 - val_accuracy: 0.5427 - val_loss: 1.1707\n",
            "Epoch 14/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6327 - loss: 0.8351 - val_accuracy: 0.5437 - val_loss: 1.2080\n",
            "Epoch 15/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6335 - loss: 0.8364 - val_accuracy: 0.5426 - val_loss: 1.2185\n",
            "Epoch 16/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6413 - loss: 0.8265 - val_accuracy: 0.5462 - val_loss: 1.2449\n",
            "Epoch 17/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6373 - loss: 0.8242 - val_accuracy: 0.5382 - val_loss: 1.3116\n",
            "Epoch 18/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6444 - loss: 0.8184 - val_accuracy: 0.5407 - val_loss: 1.2842\n",
            "Epoch 19/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6469 - loss: 0.8070 - val_accuracy: 0.5392 - val_loss: 1.3062\n",
            "Epoch 20/20\n",
            "\u001b[1m1582/1582\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.6467 - loss: 0.8055 - val_accuracy: 0.5386 - val_loss: 1.3066\n",
            "Neural Network - Final Training Accuracy: 0.6405\n",
            "Neural Network - Final Validation Accuracy: 0.5386\n",
            "Random Forest - Final Training Accuracy: 0.7617\n",
            "Random Forest - Final Validation Accuracy: 0.5381\n",
            "\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step\n",
            "\u001b[1m869/869\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    }
  ]
}